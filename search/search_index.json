{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"A Python package for efficient processing of cubic earth observation (EO) data \ud83d\ude80 GitHub : https://github.com/juliocontrerash/cubexpress/ \ud83c\udf10 PyPI : https://pypi.org/project/cubexpress/ \ud83d\udee0\ufe0f Overview \ud83d\udcca CubeXpress is a Python package designed to simplify and accelerate the process of working with Google Earth Engine (GEE) data cubes. With features like multi-threaded downloads, automatic subdivision of large requests, and direct pixel-level computations on GEE, CubeXpress helps you handle massive datasets with ease. Key Features \u2728 Fast Image and Collection Downloads Retrieve single images or entire collections at once, taking advantage of multi-threaded requests. Automatic Tiling Large images are split (\"quadsplit\") into smaller sub-tiles, preventing errors with GEE\u2019s size limits. Direct Pixel Computations Perform computations (e.g., band math) directly on GEE, then fetch results in a single step. Scalable & Efficient Optimized memory usage and parallelism let you handle complex tasks in big data environments. Installation \u2699\ufe0f Install the latest version from PyPI: pip install cubexpress Note : You need a valid Google Earth Engine account and earthengine-api installed ( pip install earthengine-api ). Also run ee.Initialize() before using CubeXpress. Basic Usage Download a single ee.Image import ee import cubexpress # Initialize Earth Engine ee . Initialize ( project = \"your-project-id\" ) # Create a raster transform geotransform = cubexpress . lonlat2rt ( lon =- 76.5 , lat =- 9.5 , edge_size = 128 , # Width=Height=128 pixels scale = 90 # 90m resolution ) # Define a single Request request = cubexpress . Request ( id = \"dem_test\" , raster_transform = geotransform , bands = [ \"elevation\" ], image = \"NASA/NASADEM_HGT/001\" # Note: you can wrap with ee.Image(\"NASA/NASADEM_HGT/001\").divide(10000) if needed # Build the RequestSet cube_requests = cubexpress . RequestSet ( requestset = [ request ]) # Download with multi-threading cubexpress . getcube ( request = cube_requests , output_path = \"output_dem\" , nworkers = 4 , max_deep_level = 5 ) This will create a GeoTIFF named dem_test.tif in the output_dem folder, containing the elevation band. Download pixel values from an ee.ImageCollection You can fetch multiple images by constructing a RequestSet with several Request objects. For example, filter Sentinel-2 images near a point: import ee import cubexpress ee . Initialize ( project = \"your-project-id\" ) # Filter a Sentinel-2 collection point = ee . Geometry . Point ([ - 97.59 , 33.37 ]) collection = ee . ImageCollection ( \"COPERNICUS/S2_SR_HARMONIZED\" ) \\ . filterBounds ( point ) \\ . filterDate ( '2024-01-01' , '2024-01-31' ) # Extract image IDs image_ids = collection . aggregate_array ( 'system:id' ) . getInfo () # Set the geotransform geotransform = cubexpress . lonlat2rt ( lon =- 97.59 , lat = 33.37 , edge_size = 512 , scale = 10 ) # Build multiple requests requests = [ cubexpress . Request ( id = f \"s2test_ { i } \" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = image_id # Note: you can wrap with ee.Image(image_id).divide(10000) if needed ) for i , image_id in enumerate ( image_ids ) ] # Create the RequestSet cube_requests = cubexpress . RequestSet ( requestset = requests ) # Download cubexpress . getcube ( request = cube_requests , output_path = \"output_sentinel\" , nworkers = 4 , max_deep_level = 5 ) Process and extract a pixel from an ee.Image If you provide an ee.Image with custom calculations (e.g., .divide(10000) , .normalizedDifference(...) ), CubeXpress can run those on GEE, then download the result. For large results, it automatically splits the image into sub-tiles. import ee import cubexpress ee . Initialize ( project = \"your-project-id\" ) # Example: NDVI from Sentinel-2 image = ee . Image ( \"COPERNICUS/S2_HARMONIZED/20170804T154911_20170804T155116_T18SUJ\" ) \\ . normalizedDifference ([ \"B8\" , \"B4\" ]) \\ . rename ( \"NDVI\" ) geotransform = cubexpress . lonlat2rt ( lon =- 76.59 , lat = 38.89 , edge_size = 256 , scale = 10 ) request = cubexpress . Request ( id = \"ndvi_test\" , raster_transform = geotransform , bands = [ \"NDVI\" ], image = image # custom expression ) cube_requests = cubexpress . RequestSet ( requestset = [ request ]) cubexpress . getcube ( request = cube_requests , output_path = \"output_ndvi\" , nworkers = 2 , max_deep_level = 5 ) Advanced Usage Same Set of Sentinel-2 Images for Multiple Points Below is a advanced example demonstrating how to work with multiple points and a Sentinel-2 image collection in one script. We first create a global collection but then filter it on a point-by-point basis, extracting only the images that intersect each coordinate. Finally, we download them in parallel using CubeXpress . import ee import cubexpress # Initialize Earth Engine with your project ee . Initialize ( project = \"your-project-id\" ) # Define multiple points (longitude, latitude) points = [ ( - 97.64 , 33.37 ), ( - 97.59 , 33.37 ) ] # Start with a broad Sentinel-2 collection collection = ( ee . ImageCollection ( \"COPERNICUS/S2_SR_HARMONIZED\" ) . filterDate ( \"2024-01-01\" , \"2024-01-31\" ) ) # Build a list of Request objects requestset = [] for i , ( lon , lat ) in enumerate ( points ): # Create a point geometry for the current coordinates point_geom = ee . Geometry . Point ([ lon , lat ]) collection_filtered = collection . filterBounds ( point_geom ) # Convert the filtered collection into a list of asset IDs image_ids = collection_filtered . aggregate_array ( \"system:id\" ) . getInfo () # Define a geotransform for this point geotransform = cubexpress . lonlat2rt ( lon = lon , lat = lat , edge_size = 512 , # Adjust the image size in pixels scale = 10 # 10m resolution for Sentinel-2 ) # Create one Request per image found for this point requestset . extend ([ cubexpress . Request ( id = f \"s2test_ { i } _ { idx } \" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = image_id ) for idx , image_id in enumerate ( image_ids ) ]) # Combine into a RequestSet cube_requests = cubexpress . RequestSet ( requestset = requestset ) # Download everything in parallel results = cubexpress . getcube ( request = cube_requests , nworkers = 4 , output_path = \"images_s2\" , max_deep_level = 5 ) print ( \"Downloaded files:\" , results ) How it works : 1. Points: We define multiple coordinates in points . 2. Global collection: We retrieve a broad Sentinel-2 collection covering the desired date range. 3. Per-point filter: For each point, we call .filterBounds(...) to get only images intersecting that location. 4. Geotransform: We create a local geotransform ( edge_size , scale ) defining the spatial extent and resolution around each point. 5. Requests: Each point-image pair becomes a Request , stored in a single list. 6. Parallel download: With cubexpress.getcube() , all requests are fetched simultaneously, automatically splitting large outputs into sub-tiles if needed (up to max_deep_level ). Contributing Contributions are welcome! Please open an issue or submit a pull request with proposed changes or bug fixes. License This project is licensed under the MIT License . Built with \ud83c\udf0e and \u2764\ufe0f by the CubeXpress team","title":"Index"},{"location":"index.html#_1","text":"A Python package for efficient processing of cubic earth observation (EO) data \ud83d\ude80 GitHub : https://github.com/juliocontrerash/cubexpress/ \ud83c\udf10 PyPI : https://pypi.org/project/cubexpress/ \ud83d\udee0\ufe0f","title":""},{"location":"index.html#overview","text":"CubeXpress is a Python package designed to simplify and accelerate the process of working with Google Earth Engine (GEE) data cubes. With features like multi-threaded downloads, automatic subdivision of large requests, and direct pixel-level computations on GEE, CubeXpress helps you handle massive datasets with ease.","title":"Overview \ud83d\udcca"},{"location":"index.html#key-features","text":"Fast Image and Collection Downloads Retrieve single images or entire collections at once, taking advantage of multi-threaded requests. Automatic Tiling Large images are split (\"quadsplit\") into smaller sub-tiles, preventing errors with GEE\u2019s size limits. Direct Pixel Computations Perform computations (e.g., band math) directly on GEE, then fetch results in a single step. Scalable & Efficient Optimized memory usage and parallelism let you handle complex tasks in big data environments.","title":"Key Features \u2728"},{"location":"index.html#installation","text":"Install the latest version from PyPI: pip install cubexpress Note : You need a valid Google Earth Engine account and earthengine-api installed ( pip install earthengine-api ). Also run ee.Initialize() before using CubeXpress.","title":"Installation \u2699\ufe0f"},{"location":"index.html#basic-usage","text":"","title":"Basic Usage"},{"location":"index.html#download-a-single-eeimage","text":"import ee import cubexpress # Initialize Earth Engine ee . Initialize ( project = \"your-project-id\" ) # Create a raster transform geotransform = cubexpress . lonlat2rt ( lon =- 76.5 , lat =- 9.5 , edge_size = 128 , # Width=Height=128 pixels scale = 90 # 90m resolution ) # Define a single Request request = cubexpress . Request ( id = \"dem_test\" , raster_transform = geotransform , bands = [ \"elevation\" ], image = \"NASA/NASADEM_HGT/001\" # Note: you can wrap with ee.Image(\"NASA/NASADEM_HGT/001\").divide(10000) if needed # Build the RequestSet cube_requests = cubexpress . RequestSet ( requestset = [ request ]) # Download with multi-threading cubexpress . getcube ( request = cube_requests , output_path = \"output_dem\" , nworkers = 4 , max_deep_level = 5 ) This will create a GeoTIFF named dem_test.tif in the output_dem folder, containing the elevation band.","title":"Download a single ee.Image"},{"location":"index.html#download-pixel-values-from-an-eeimagecollection","text":"You can fetch multiple images by constructing a RequestSet with several Request objects. For example, filter Sentinel-2 images near a point: import ee import cubexpress ee . Initialize ( project = \"your-project-id\" ) # Filter a Sentinel-2 collection point = ee . Geometry . Point ([ - 97.59 , 33.37 ]) collection = ee . ImageCollection ( \"COPERNICUS/S2_SR_HARMONIZED\" ) \\ . filterBounds ( point ) \\ . filterDate ( '2024-01-01' , '2024-01-31' ) # Extract image IDs image_ids = collection . aggregate_array ( 'system:id' ) . getInfo () # Set the geotransform geotransform = cubexpress . lonlat2rt ( lon =- 97.59 , lat = 33.37 , edge_size = 512 , scale = 10 ) # Build multiple requests requests = [ cubexpress . Request ( id = f \"s2test_ { i } \" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = image_id # Note: you can wrap with ee.Image(image_id).divide(10000) if needed ) for i , image_id in enumerate ( image_ids ) ] # Create the RequestSet cube_requests = cubexpress . RequestSet ( requestset = requests ) # Download cubexpress . getcube ( request = cube_requests , output_path = \"output_sentinel\" , nworkers = 4 , max_deep_level = 5 )","title":"Download pixel values from an ee.ImageCollection"},{"location":"index.html#process-and-extract-a-pixel-from-an-eeimage","text":"If you provide an ee.Image with custom calculations (e.g., .divide(10000) , .normalizedDifference(...) ), CubeXpress can run those on GEE, then download the result. For large results, it automatically splits the image into sub-tiles. import ee import cubexpress ee . Initialize ( project = \"your-project-id\" ) # Example: NDVI from Sentinel-2 image = ee . Image ( \"COPERNICUS/S2_HARMONIZED/20170804T154911_20170804T155116_T18SUJ\" ) \\ . normalizedDifference ([ \"B8\" , \"B4\" ]) \\ . rename ( \"NDVI\" ) geotransform = cubexpress . lonlat2rt ( lon =- 76.59 , lat = 38.89 , edge_size = 256 , scale = 10 ) request = cubexpress . Request ( id = \"ndvi_test\" , raster_transform = geotransform , bands = [ \"NDVI\" ], image = image # custom expression ) cube_requests = cubexpress . RequestSet ( requestset = [ request ]) cubexpress . getcube ( request = cube_requests , output_path = \"output_ndvi\" , nworkers = 2 , max_deep_level = 5 )","title":"Process and extract a pixel from an ee.Image"},{"location":"index.html#advanced-usage","text":"","title":"Advanced Usage"},{"location":"index.html#same-set-of-sentinel-2-images-for-multiple-points","text":"Below is a advanced example demonstrating how to work with multiple points and a Sentinel-2 image collection in one script. We first create a global collection but then filter it on a point-by-point basis, extracting only the images that intersect each coordinate. Finally, we download them in parallel using CubeXpress . import ee import cubexpress # Initialize Earth Engine with your project ee . Initialize ( project = \"your-project-id\" ) # Define multiple points (longitude, latitude) points = [ ( - 97.64 , 33.37 ), ( - 97.59 , 33.37 ) ] # Start with a broad Sentinel-2 collection collection = ( ee . ImageCollection ( \"COPERNICUS/S2_SR_HARMONIZED\" ) . filterDate ( \"2024-01-01\" , \"2024-01-31\" ) ) # Build a list of Request objects requestset = [] for i , ( lon , lat ) in enumerate ( points ): # Create a point geometry for the current coordinates point_geom = ee . Geometry . Point ([ lon , lat ]) collection_filtered = collection . filterBounds ( point_geom ) # Convert the filtered collection into a list of asset IDs image_ids = collection_filtered . aggregate_array ( \"system:id\" ) . getInfo () # Define a geotransform for this point geotransform = cubexpress . lonlat2rt ( lon = lon , lat = lat , edge_size = 512 , # Adjust the image size in pixels scale = 10 # 10m resolution for Sentinel-2 ) # Create one Request per image found for this point requestset . extend ([ cubexpress . Request ( id = f \"s2test_ { i } _ { idx } \" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = image_id ) for idx , image_id in enumerate ( image_ids ) ]) # Combine into a RequestSet cube_requests = cubexpress . RequestSet ( requestset = requestset ) # Download everything in parallel results = cubexpress . getcube ( request = cube_requests , nworkers = 4 , output_path = \"images_s2\" , max_deep_level = 5 ) print ( \"Downloaded files:\" , results ) How it works : 1. Points: We define multiple coordinates in points . 2. Global collection: We retrieve a broad Sentinel-2 collection covering the desired date range. 3. Per-point filter: For each point, we call .filterBounds(...) to get only images intersecting that location. 4. Geotransform: We create a local geotransform ( edge_size , scale ) defining the spatial extent and resolution around each point. 5. Requests: Each point-image pair becomes a Request , stored in a single list. 6. Parallel download: With cubexpress.getcube() , all requests are fetched simultaneously, automatically splitting large outputs into sub-tiles if needed (up to max_deep_level ).","title":"Same Set of Sentinel-2 Images for Multiple Points"},{"location":"index.html#contributing","text":"Contributions are welcome! Please open an issue or submit a pull request with proposed changes or bug fixes.","title":"Contributing"},{"location":"index.html#license","text":"This project is licensed under the MIT License . Built with \ud83c\udf0e and \u2764\ufe0f by the CubeXpress team","title":"License"},{"location":"CONTRIBUTING.html","text":"Contributing to CubeXpress Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/JulioContrerasH/CubeXpress/issues If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement a fix for it. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation Cookiecutter PyPackage could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback The best way to send feedback is to file an issue at https://github.com/JulioContrerasH/CubeXpress/issues. If you are proposing a new feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! Ready to contribute? Here's how to set up CubeXpress for local development. Please note this documentation assumes you already have poetry and Git installed and ready to go. Fork the CubeXpress repo on GitHub. Clone your fork locally: cd <directory_in_which_repo_should_be_created> git clone git@github.com:YOUR_NAME/CubeXpress.git Now we need to install the environment. Navigate into the directory cd CubeXpress If you are using pyenv , select a version to use locally. (See installed versions with pyenv versions ) pyenv local <x.y.z> Then, install and activate the environment with: poetry install poetry shell Install pre-commit to run linters/formatters at commit time: poetry run pre-commit install Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Don't forget to add test cases for your added functionality to the tests directory. When you're done making changes, check that your changes pass the formatting tests. make check Now, validate that all unit tests are passing: make test Before raising a pull request you should also run tox. This will run the tests across different versions of Python: tox This requires you to have multiple versions of python installed. This step is also triggered in the CI/CD pipeline, so you could also choose to skip this step locally. Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md .","title":"Contributing"},{"location":"CONTRIBUTING.html#contributing-to-cubexpress","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing to CubeXpress"},{"location":"CONTRIBUTING.html#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"CONTRIBUTING.html#report-bugs","text":"Report bugs at https://github.com/JulioContrerasH/CubeXpress/issues If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"CONTRIBUTING.html#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement a fix for it.","title":"Fix Bugs"},{"location":"CONTRIBUTING.html#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"CONTRIBUTING.html#write-documentation","text":"Cookiecutter PyPackage could always use more documentation, whether as part of the official docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"CONTRIBUTING.html#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/JulioContrerasH/CubeXpress/issues. If you are proposing a new feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"CONTRIBUTING.html#get-started","text":"Ready to contribute? Here's how to set up CubeXpress for local development. Please note this documentation assumes you already have poetry and Git installed and ready to go. Fork the CubeXpress repo on GitHub. Clone your fork locally: cd <directory_in_which_repo_should_be_created> git clone git@github.com:YOUR_NAME/CubeXpress.git Now we need to install the environment. Navigate into the directory cd CubeXpress If you are using pyenv , select a version to use locally. (See installed versions with pyenv versions ) pyenv local <x.y.z> Then, install and activate the environment with: poetry install poetry shell Install pre-commit to run linters/formatters at commit time: poetry run pre-commit install Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. Don't forget to add test cases for your added functionality to the tests directory. When you're done making changes, check that your changes pass the formatting tests. make check Now, validate that all unit tests are passing: make test Before raising a pull request you should also run tox. This will run the tests across different versions of Python: tox This requires you to have multiple versions of python installed. This step is also triggered in the CI/CD pipeline, so you could also choose to skip this step locally. Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"CONTRIBUTING.html#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md .","title":"Pull Request Guidelines"},{"location":"docs/COMPARATION.html","text":"Comparison of methods This section compares the methods ee.data.computePixels and ee.data.getPixels with more traditional data download methods, such as Export and getDownloadUrl , focusing on their application in deep learning workflows and overall performance.","title":"Comparison of methods"},{"location":"docs/COMPARATION.html#comparison-of-methods","text":"This section compares the methods ee.data.computePixels and ee.data.getPixels with more traditional data download methods, such as Export and getDownloadUrl , focusing on their application in deep learning workflows and overall performance.","title":"Comparison of methods"},{"location":"docs/functions.html","text":"Functions in CubeXpress This section explores how the CubeXpress package streamlines working with satellite imagery and geospatial data in Google Earth Engine (GEE). CubeXpress provides a simple, unified API for creating download requests, handling automatic tiling of large images, and performing pixel-level computations. 1. Introduction to cubexpress The CubeXpress package is designed to simplify and accelerate the retrieval and processing of large geospatial datasets from Google Earth Engine (GEE). Key capabilities include: Unified \u201cRequest\u201d and \u201cRequestSet\u201d model: Define one or many image download requests with minimal code, then process them in bulk. Automatic sub-tiling: If an image exceeds GEE\u2019s size limits, CubeXpress will automatically split (\u201cquad-split\u201d) the request into smaller tiles until it succeeds. Parallel downloads: Multiple images can be retrieved concurrently using the nworkers parameter, greatly improving performance. Flexible inputs: Pass an ee.Image object (including complex expressions like .normalizedDifference(...) or .divide(...) ). Or simply provide a string with the asset ID (e.g., \"COPERNICUS/S2_SR_HARMONIZED/...\" ). 2. Main Classes and Functions Request A single image download specification. Parameters : id : Unique identifier for the request (used for naming output files). raster_transform : Spatial metadata, typically created via lonlat2rt(...) . image : Can be an ee.Image (serialized internally) or a string asset ID. bands : List of band names to extract. Example : request = cubexpress . Request ( id = \"my_image\" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = \"COPERNICUS/S2_SR_HARMONIZED/20240105T...\" ) RequestSet A collection of Request objects, validated and prepared for download. Creates an internal DataFrame with all request details (the \u201cmanifest\u201d). Example : requests = [ request1 , request2 , ... ] request_set = cubexpress . RequestSet ( requestset = requests ) getcube() The main download function. It reads the manifest from a RequestSet , calls GEE\u2019s internal APIs ( getPixels / computePixels ), and writes GeoTIFFs to disk. Arguments : request : The RequestSet to process. output_path : Directory for saving the resulting GeoTIFF files. nworkers : Number of parallel threads (workers). max_deep_level : Maximum recursion depth if sub-tiling is required. Returns : A list of pathlib.Path objects pointing to the downloaded files.","title":"Functions in **CubeXpress**"},{"location":"docs/functions.html#functions-in-cubexpress","text":"This section explores how the CubeXpress package streamlines working with satellite imagery and geospatial data in Google Earth Engine (GEE). CubeXpress provides a simple, unified API for creating download requests, handling automatic tiling of large images, and performing pixel-level computations.","title":"Functions in CubeXpress"},{"location":"docs/functions.html#1-introduction-to-cubexpress","text":"The CubeXpress package is designed to simplify and accelerate the retrieval and processing of large geospatial datasets from Google Earth Engine (GEE). Key capabilities include: Unified \u201cRequest\u201d and \u201cRequestSet\u201d model: Define one or many image download requests with minimal code, then process them in bulk. Automatic sub-tiling: If an image exceeds GEE\u2019s size limits, CubeXpress will automatically split (\u201cquad-split\u201d) the request into smaller tiles until it succeeds. Parallel downloads: Multiple images can be retrieved concurrently using the nworkers parameter, greatly improving performance. Flexible inputs: Pass an ee.Image object (including complex expressions like .normalizedDifference(...) or .divide(...) ). Or simply provide a string with the asset ID (e.g., \"COPERNICUS/S2_SR_HARMONIZED/...\" ).","title":"1. Introduction to cubexpress"},{"location":"docs/functions.html#2-main-classes-and-functions","text":"","title":"2. Main Classes and Functions"},{"location":"docs/functions.html#request","text":"A single image download specification. Parameters : id : Unique identifier for the request (used for naming output files). raster_transform : Spatial metadata, typically created via lonlat2rt(...) . image : Can be an ee.Image (serialized internally) or a string asset ID. bands : List of band names to extract. Example : request = cubexpress . Request ( id = \"my_image\" , raster_transform = geotransform , bands = [ \"B4\" , \"B3\" , \"B2\" ], image = \"COPERNICUS/S2_SR_HARMONIZED/20240105T...\" )","title":"Request"},{"location":"docs/functions.html#requestset","text":"A collection of Request objects, validated and prepared for download. Creates an internal DataFrame with all request details (the \u201cmanifest\u201d). Example : requests = [ request1 , request2 , ... ] request_set = cubexpress . RequestSet ( requestset = requests )","title":"RequestSet"},{"location":"docs/functions.html#getcube","text":"The main download function. It reads the manifest from a RequestSet , calls GEE\u2019s internal APIs ( getPixels / computePixels ), and writes GeoTIFFs to disk. Arguments : request : The RequestSet to process. output_path : Directory for saving the resulting GeoTIFF files. nworkers : Number of parallel threads (workers). max_deep_level : Maximum recursion depth if sub-tiling is required. Returns : A list of pathlib.Path objects pointing to the downloaded files.","title":"getcube()"},{"location":"docs/process.html","text":"Recursion, Concurrency, and Parallelism in CubeXpress Modern geospatial workflows often involve large datasets and computationally intensive tasks. CubeXpress leverages recursion, concurrency, and parallelism to optimize these operations and handle massive image retrievals from Google Earth Engine (GEE). This section provides a concise overview of each concept and how CubeXpress implements them. 1. Concurrency \u2699\ufe0f Definition : Concurrency refers to managing multiple tasks such that they all make progress over the same time period\u2014even if they\u2019re not strictly executing at the exact same moment. Usage in CubeXpress : The package uses ThreadPoolExecutor to coordinate concurrent downloads inside getcube() . By submitting multiple requests for images simultaneously, CubeXpress maximizes CPU and network usage, drastically reducing the overall download duration. 2. Parallelism \ud83d\udda5\ufe0f Definition : Parallelism describes running multiple tasks truly at the same time, often on separate CPU cores. Usage in CubeXpress : On multi-core systems, thread-based parallelism ensures that multiple image downloads or computations happen concurrently\u2014spreading the load across available cores. This can significantly improve performance when dealing with large or numerous GEE images. Concurrency vs. Parallelism Concurrency : Tasks appear to run simultaneously but may share a single core, each making progress in turn (time-slicing). Parallelism : Tasks literally run at the same time on different CPU cores. In practice, CubeXpress exploits both\u2014concurrent scheduling of tasks and true parallel execution if multiple CPU cores are available. 3. Recursion \ud83d\udd04 Definition : Recursion is the technique of breaking down a problem into smaller, more manageable subproblems, where a function calls itself until reaching a base case. Usage in CubeXpress : When an image exceeds GEE\u2019s size limits, CubeXpress automatically splits (quad-splits) it into smaller tiles. This splitting continues recursively ( max_deep_level ) until each tile is sufficiently small to download successfully. This approach ensures you can handle massive requests without manually dividing your region of interest. CubeXpress enables highly scalable and robust geospatial workflows with Google Earth Engine. These concepts work together to optimize performance, reduce download times, and handle exceptionally large datasets, giving you a seamless experience when building geospatial data cubes.","title":"Recursion, Concurrency, and Parallelism in **CubeXpress**"},{"location":"docs/process.html#recursion-concurrency-and-parallelism-in-cubexpress","text":"Modern geospatial workflows often involve large datasets and computationally intensive tasks. CubeXpress leverages recursion, concurrency, and parallelism to optimize these operations and handle massive image retrievals from Google Earth Engine (GEE). This section provides a concise overview of each concept and how CubeXpress implements them.","title":"Recursion, Concurrency, and Parallelism in CubeXpress"},{"location":"docs/process.html#1-concurrency","text":"Definition : Concurrency refers to managing multiple tasks such that they all make progress over the same time period\u2014even if they\u2019re not strictly executing at the exact same moment. Usage in CubeXpress : The package uses ThreadPoolExecutor to coordinate concurrent downloads inside getcube() . By submitting multiple requests for images simultaneously, CubeXpress maximizes CPU and network usage, drastically reducing the overall download duration.","title":"1. Concurrency \u2699\ufe0f"},{"location":"docs/process.html#2-parallelism","text":"Definition : Parallelism describes running multiple tasks truly at the same time, often on separate CPU cores. Usage in CubeXpress : On multi-core systems, thread-based parallelism ensures that multiple image downloads or computations happen concurrently\u2014spreading the load across available cores. This can significantly improve performance when dealing with large or numerous GEE images.","title":"2. Parallelism \ud83d\udda5\ufe0f"},{"location":"docs/process.html#concurrency-vs-parallelism","text":"Concurrency : Tasks appear to run simultaneously but may share a single core, each making progress in turn (time-slicing). Parallelism : Tasks literally run at the same time on different CPU cores. In practice, CubeXpress exploits both\u2014concurrent scheduling of tasks and true parallel execution if multiple CPU cores are available.","title":"Concurrency vs. Parallelism"},{"location":"docs/process.html#3-recursion","text":"Definition : Recursion is the technique of breaking down a problem into smaller, more manageable subproblems, where a function calls itself until reaching a base case. Usage in CubeXpress : When an image exceeds GEE\u2019s size limits, CubeXpress automatically splits (quad-splits) it into smaller tiles. This splitting continues recursively ( max_deep_level ) until each tile is sufficiently small to download successfully. This approach ensures you can handle massive requests without manually dividing your region of interest. CubeXpress enables highly scalable and robust geospatial workflows with Google Earth Engine. These concepts work together to optimize performance, reduce download times, and handle exceptionally large datasets, giving you a seamless experience when building geospatial data cubes.","title":"3. Recursion \ud83d\udd04"},{"location":"docs/Comparation/comparison.html","text":"\ud83c\udd9a Comparison with traditional methods ( Export and getDownloadUrl ) Export : \ud83d\udce6 Useful for exporting large datasets or entire collections, but involves significant backend overhead to start and manage export tasks, often resulting in longer wait times for the data to be ready. This is less ideal for real-time or high-frequency deep learning data preparation where immediate data access is critical. getDownloadUrl : \ud83c\udf10 Provides direct download via a URL but lacks server-side pre-processing capabilities, requiring all data manipulation to be performed locally. This can be time-consuming and resource-intensive, especially for deep learning tasks that involve large amounts of data and require extensive preprocessing. Efficiency gains with getPixels and computePixels : computePixels can provide significant speed improvements by pre-processing data directly on the server, reducing both the download size and the computational load on local systems, which is essential for deep learning workflows that involve large datasets. Reduced candwidth usage : \ud83d\udcc9 Since only the necessary data is downloaded, bandwidth usage is minimized, which is particularly important when handling large amounts of data for training deep learning models. \ud83d\udcca Effectiveness comparison Method Download speed Pre-processing Use case ee.data.getPixels \u26a1 Fast (minimal server processing) \u274c No Retrieving unprocessed satellite images for flexible local processing in deep learning. ee.data.computePixels \ud83d\udd52 Moderate (due to server processing) \u2705 Yes Applying preprocessing tasks (e.g., normalization, cloud masking) directly on the server before downloading for deep learning workflows. Export \ud83d\udc22 Slow due to backend processing and queuing \u2705 Yes (on the server) Suitable for exporting large datasets or entire collections, but less efficient for high-frequency data preparation in deep learning. getDownloadUrl \ud83d\udd51 Moderate (depending on dataset size) \u274c No Direct download of specific images or data selections without pre-processing, requiring all manipulations to be done locally.","title":"Comparison"},{"location":"docs/Comparation/comparison.html#comparison-with-traditional-methods-export-and-getdownloadurl","text":"Export : \ud83d\udce6 Useful for exporting large datasets or entire collections, but involves significant backend overhead to start and manage export tasks, often resulting in longer wait times for the data to be ready. This is less ideal for real-time or high-frequency deep learning data preparation where immediate data access is critical. getDownloadUrl : \ud83c\udf10 Provides direct download via a URL but lacks server-side pre-processing capabilities, requiring all data manipulation to be performed locally. This can be time-consuming and resource-intensive, especially for deep learning tasks that involve large amounts of data and require extensive preprocessing. Efficiency gains with getPixels and computePixels : computePixels can provide significant speed improvements by pre-processing data directly on the server, reducing both the download size and the computational load on local systems, which is essential for deep learning workflows that involve large datasets. Reduced candwidth usage : \ud83d\udcc9 Since only the necessary data is downloaded, bandwidth usage is minimized, which is particularly important when handling large amounts of data for training deep learning models.","title":"\ud83c\udd9a Comparison with traditional methods (Export and getDownloadUrl)"},{"location":"docs/Comparation/comparison.html#effectiveness-comparison","text":"Method Download speed Pre-processing Use case ee.data.getPixels \u26a1 Fast (minimal server processing) \u274c No Retrieving unprocessed satellite images for flexible local processing in deep learning. ee.data.computePixels \ud83d\udd52 Moderate (due to server processing) \u2705 Yes Applying preprocessing tasks (e.g., normalization, cloud masking) directly on the server before downloading for deep learning workflows. Export \ud83d\udc22 Slow due to backend processing and queuing \u2705 Yes (on the server) Suitable for exporting large datasets or entire collections, but less efficient for high-frequency data preparation in deep learning. getDownloadUrl \ud83d\udd51 Moderate (depending on dataset size) \u274c No Direct download of specific images or data selections without pre-processing, requiring all manipulations to be done locally.","title":"\ud83d\udcca Effectiveness comparison"},{"location":"docs/Comparation/conclusion.html","text":"\ud83c\udfc1 Conclusion For deep learning workflows that require efficient data handling and pre-processing, ee.data.getPixels and ee.data.computePixels offer significant improvements over more traditional methods like Export and getDownloadUrl . These newer methods reduce local processing needs and bandwidth usage by leveraging server-side capabilities, making them ideal for preparing large-scale datasets needed for training deep learning models. Choosing the right method depends on the specific use case, data size, and required preprocessing steps.","title":"Conclusion"},{"location":"docs/Comparation/conclusion.html#conclusion","text":"For deep learning workflows that require efficient data handling and pre-processing, ee.data.getPixels and ee.data.computePixels offer significant improvements over more traditional methods like Export and getDownloadUrl . These newer methods reduce local processing needs and bandwidth usage by leveraging server-side capabilities, making them ideal for preparing large-scale datasets needed for training deep learning models. Choosing the right method depends on the specific use case, data size, and required preprocessing steps.","title":"\ud83c\udfc1 Conclusion"},{"location":"docs/Comparation/eedatacomputePixels.html","text":"\ud83e\udde0 ee.data.computePixels Purpose : \ud83d\udda5\ufe0f Allows applying computations to the image data on GEE servers before downloading. Typical use : \ud83e\udd16 Ideal for deep learning workflows where pre-processing, such as normalization, cloud masking, or NDVI calculations, is needed directly on the server before downloading to reduce local computational load and data size. Advantages : Pre-processing on the server : \ud83d\udee0\ufe0f Significantly reduces the amount of data to download by performing operations on the server (e.g., filtering, image enhancement). Improved speed and efficiency : \ud83d\ude80 Saves local processing time and resources by downloading pre-processed images, which is particularly beneficial for deep learning models that require specific input formats or preprocessing. Optimized data handling : \ud83d\udcca Minimizes bandwidth usage and optimizes data handling by transferring only the necessary processed data, making it ideal for scenarios with limited bandwidth or when handling large-scale datasets.","title":"Compute"},{"location":"docs/Comparation/eedatacomputePixels.html#eedatacomputepixels","text":"Purpose : \ud83d\udda5\ufe0f Allows applying computations to the image data on GEE servers before downloading. Typical use : \ud83e\udd16 Ideal for deep learning workflows where pre-processing, such as normalization, cloud masking, or NDVI calculations, is needed directly on the server before downloading to reduce local computational load and data size. Advantages : Pre-processing on the server : \ud83d\udee0\ufe0f Significantly reduces the amount of data to download by performing operations on the server (e.g., filtering, image enhancement). Improved speed and efficiency : \ud83d\ude80 Saves local processing time and resources by downloading pre-processed images, which is particularly beneficial for deep learning models that require specific input formats or preprocessing. Optimized data handling : \ud83d\udcca Minimizes bandwidth usage and optimizes data handling by transferring only the necessary processed data, making it ideal for scenarios with limited bandwidth or when handling large-scale datasets.","title":"\ud83e\udde0 ee.data.computePixels"},{"location":"docs/Comparation/eedatagetPixels.html","text":"\u2728 ee.data.getPixels Purpose : \ud83d\udce5 Downloads raw image data without additional processing. Typical use : \ud83d\udef0\ufe0f Ideal for obtaining unmodified satellite images for further analysis on local systems, especially when large amounts of raw data are needed quickly for training machine learning models. Advantages : \u26a1 Faster download time due to minimal server-side processing. \u23f1\ufe0f Quicker downloads for simple images or when no additional processing is required. \ud83d\udcbe Suitable for workflows where large datasets are required for deep learning, allowing flexibility in local preprocessing and augmentation.","title":"Get"},{"location":"docs/Comparation/eedatagetPixels.html#eedatagetpixels","text":"Purpose : \ud83d\udce5 Downloads raw image data without additional processing. Typical use : \ud83d\udef0\ufe0f Ideal for obtaining unmodified satellite images for further analysis on local systems, especially when large amounts of raw data are needed quickly for training machine learning models. Advantages : \u26a1 Faster download time due to minimal server-side processing. \u23f1\ufe0f Quicker downloads for simple images or when no additional processing is required. \ud83d\udcbe Suitable for workflows where large datasets are required for deep learning, allowing flexibility in local preprocessing and augmentation.","title":"\u2728 ee.data.getPixels"},{"location":"docs/Functions/download_data.html","text":"Download data \u2b07\ufe0f \ud83c\udf10 getPixels and computePixels Description : These functions download images or image collections as GeoTIFF files using ee.data.getPixels or ee.data.computePixels . Arguments : \ud83d\uddc2\ufe0f table : A Data6+3Frame containing the metadata for the download. \u2699\ufe0f nworkers : Number of threawds (workers) for concurrent downloading. \ud83d\udd04 deep_level / max_deep_level : Maximum depth for recursion when downloading large images. Returns : A list of paths \ud83d\udcc2 to the downloaded GeoTIFF files. fastcubo . getPixels ( table , nworkers = 4 , output_path = \"demo1\" ) # for table in query_getPixels_image fastcubo . computePixels ( table , nworkers = 4 , output_path = \"demo3\" ) # for table in query_computePixels_image","title":"**Download data** \u2b07\ufe0f"},{"location":"docs/Functions/download_data.html#download-data","text":"","title":"Download data \u2b07\ufe0f"},{"location":"docs/Functions/download_data.html#getpixels-and-computepixels","text":"Description : These functions download images or image collections as GeoTIFF files using ee.data.getPixels or ee.data.computePixels . Arguments : \ud83d\uddc2\ufe0f table : A Data6+3Frame containing the metadata for the download. \u2699\ufe0f nworkers : Number of threawds (workers) for concurrent downloading. \ud83d\udd04 deep_level / max_deep_level : Maximum depth for recursion when downloading large images. Returns : A list of paths \ud83d\udcc2 to the downloaded GeoTIFF files. fastcubo . getPixels ( table , nworkers = 4 , output_path = \"demo1\" ) # for table in query_getPixels_image fastcubo . computePixels ( table , nworkers = 4 , output_path = \"demo3\" ) # for table in query_computePixels_image","title":"\ud83c\udf10 getPixels and computePixels"},{"location":"docs/Functions/process_image.html","text":"Process image \ud83d\uddbc\ufe0f \ud83d\udd0d query_computePixels_image Description : Similar to query_getPixels_image , but uses a GEE image expression ( ee.Image ) for more complex queries. Arguments : Adds expression to specify the GEE image expression. Returns : pd.DataFrame with metadata required to download processed images. table = fastcubo . query_computePixels_image ( points = [( - 76.5 , - 9.5 ), ( - 76.5 , - 10.5 ), ( - 77.5 , - 10.5 )], expression = ee . Image ( \"NASA/NASADEM_HGT/001\" ) . divide ( 1000 ), bands = [ \"elevation\" ], edge_size = 128 , resolution = 90 ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; /* Usa 'auto' en lugar de 'hidden' para permitir desplazamiento si es necesario */ white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution img_id img_date manifest outname 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PVM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PWM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T070007_20160608T103645_T39PVM.tif","title":"**Process image** \ud83d\uddbc\ufe0f"},{"location":"docs/Functions/process_image.html#process-image","text":"","title":"Process image \ud83d\uddbc\ufe0f"},{"location":"docs/Functions/process_image.html#query_computepixels_image","text":"Description : Similar to query_getPixels_image , but uses a GEE image expression ( ee.Image ) for more complex queries. Arguments : Adds expression to specify the GEE image expression. Returns : pd.DataFrame with metadata required to download processed images. table = fastcubo . query_computePixels_image ( points = [( - 76.5 , - 9.5 ), ( - 76.5 , - 10.5 ), ( - 77.5 , - 10.5 )], expression = ee . Image ( \"NASA/NASADEM_HGT/001\" ) . divide ( 1000 ), bands = [ \"elevation\" ], edge_size = 128 , resolution = 90 ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; /* Usa 'auto' en lugar de 'hidden' para permitir desplazamiento si es necesario */ white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution img_id img_date manifest outname 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PVM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PWM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T070007_20160608T103645_T39PVM.tif","title":"\ud83d\udd0d query_computePixels_image"},{"location":"docs/Functions/query_metadata.html","text":"Query metadata \ud83d\udcca \ud83d\udd0d query_getPixels_image Description : Returns a DataFrame with the metadata required to retrieve data using ee.data.getPixels . Arguments : \ud83d\udccd points : A list of tuples with coordinates (longitude, latitude). \ud83d\udcda collection : GEE collection to query. \ud83c\udfa8 bands : Image bands to query. \ud83d\uddbc\ufe0f edge_size : Size of the query square. \ud83c\udfaf resolution : Resolution of the query. Returns : pd.DataFrame containing the metadata necessary for download. table = fastcubo . query_getPixels_image ( points = [( - 76.5 , - 9.5 ), ( - 76.5 , - 10.5 ), ( - 77.5 , - 10.5 )], collection = \"NASA/NASADEM_HGT/001\" , bands = [ \"elevation\" ], edge_size = 128 , resolution = 90 ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution manifest outname -76.5 -9.5 329583.7418991233 8955272.65902687 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 329583.7418991233, 'shearY': 0, 'scaleY': -90, 'translateY': 8955272.65902687}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0000.tif -76.5 -10.5 330086.6486314098 8844673.258434067 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 330086.6486314098, 'shearY': 0, 'scaleY': -90, 'translateY': 8844673.258434067}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0001.tif -77.5 -10.5 220598.83698345214 8843976.459310157 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 220598.83698345214, 'shearY': 0, 'scaleY': -90, 'translateY': 8843976.459310157}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0002.tif \ud83d\udcc5 query_getPixels_imagecollection Description : Similar to query_getPixels_image , but works with image collections instead of individual images. Arguments : Adds data_range \u23f3 to specify the date range. Returns : pd.DataFrame \ud83d\udcdd with metadata needed to download image collections. table = fastcubo . query_getPixels_imagecollection ( point = ( 51.079225 , 10.452173 ), collection = \"COPERNICUS/S2_HARMONIZED\" , bands = [ \"B4\" , \"B3\" , \"B2\" ], data_range = [ \"2016-06-01\" , \"2017-07-01\" ], edge_size = 128 , resolution = 10 , ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution img_id img_date manifest outname 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PVM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PWM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T070007_20160608T103645_T39PVM.tif","title":"**Query metadata** \ud83d\udcca"},{"location":"docs/Functions/query_metadata.html#query-metadata","text":"","title":"Query metadata \ud83d\udcca"},{"location":"docs/Functions/query_metadata.html#query_getpixels_image","text":"Description : Returns a DataFrame with the metadata required to retrieve data using ee.data.getPixels . Arguments : \ud83d\udccd points : A list of tuples with coordinates (longitude, latitude). \ud83d\udcda collection : GEE collection to query. \ud83c\udfa8 bands : Image bands to query. \ud83d\uddbc\ufe0f edge_size : Size of the query square. \ud83c\udfaf resolution : Resolution of the query. Returns : pd.DataFrame containing the metadata necessary for download. table = fastcubo . query_getPixels_image ( points = [( - 76.5 , - 9.5 ), ( - 76.5 , - 10.5 ), ( - 77.5 , - 10.5 )], collection = \"NASA/NASADEM_HGT/001\" , bands = [ \"elevation\" ], edge_size = 128 , resolution = 90 ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution manifest outname -76.5 -9.5 329583.7418991233 8955272.65902687 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 329583.7418991233, 'shearY': 0, 'scaleY': -90, 'translateY': 8955272.65902687}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0000.tif -76.5 -10.5 330086.6486314098 8844673.258434067 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 330086.6486314098, 'shearY': 0, 'scaleY': -90, 'translateY': 8844673.258434067}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0001.tif -77.5 -10.5 220598.83698345214 8843976.459310157 EPSG:32718 NASA/NASADEM_HGT/001 elevation 128 90 {'assetId': 'NASA/NASADEM_HGT/001', 'fileFormat': 'GEO_TIFF', 'bandIds': ['elevation'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 90, 'shearX': 0, 'translateX': 220598.83698345214, 'shearY': 0, 'scaleY': -90, 'translateY': 8843976.459310157}, 'crsCode': 'EPSG:32718'}} NASA_NASADEM_HGT_001__0002.tif","title":"\ud83d\udd0d query_getPixels_image"},{"location":"docs/Functions/query_metadata.html#query_getpixels_imagecollection","text":"Description : Similar to query_getPixels_image , but works with image collections instead of individual images. Arguments : Adds data_range \u23f3 to specify the date range. Returns : pd.DataFrame \ud83d\udcdd with metadata needed to download image collections. table = fastcubo . query_getPixels_imagecollection ( point = ( 51.079225 , 10.452173 ), collection = \"COPERNICUS/S2_HARMONIZED\" , bands = [ \"B4\" , \"B3\" , \"B2\" ], data_range = [ \"2016-06-01\" , \"2017-07-01\" ], edge_size = 128 , resolution = 10 , ) print ( table ) .scrollable-table { display: block; width: 100%; overflow-x: clip; white-space: nowrap; border: 1px solid #ddd; margin-bottom: 20px; border-radius: 5px; } table { border-collapse: collapse; width: 100%; } th, td { text-align: left; padding: 8px; border-bottom: 1px solid #ddd; } lon lat x y epsg collection bands edge_size resolution img_id img_date manifest outname 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PVM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T064632_20160608T070007_T39PWM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T064632_20160608T070007_T39PWM.tif 51.079225 10.452173 508030.43405854504 1156048.1056529859 EPSG:32639 COPERNICUS/S2_HARMONIZED B4, B3, B2 128 10 COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM 2016-06-08 07:00:07 {'assetId': 'COPERNICUS/S2_HARMONIZED/20160608T070007_20160608T103645_T39PVM', 'fileFormat': 'GEO_TIFF', 'bandIds': ['B4', 'B3', 'B2'], 'grid': {'dimensions': {'width': 128, 'height': 128}, 'affineTransform': {'scaleX': 10, 'shearX': 0, 'translateX': 508030.43405854504, 'shearY': 0, 'scaleY': -10, 'translateY': 1156048.1056529859}, 'crsCode': 'EPSG:32639'}} 20160608T070007_20160608T103645_T39PVM.tif","title":"\ud83d\udcc5 query_getPixels_imagecollection"},{"location":"docs/Process/concurrency.html","text":"Concurrency \u2699\ufe0f \ud83d\udcd6 Definition : Concurrency is the management of multiple tasks at the same time. In fastcubo , it is used to handle multiple image downloads efficiently. \ud83d\udcbb Example in fastcubo : ThreadPoolExecutor is used to manage concurrent downloads in getPixels and computePixels , allowing multiple images to be downloaded simultaneously, maximizing CPU and network usage.","title":"**Concurrency** \u2699\ufe0f"},{"location":"docs/Process/concurrency.html#concurrency","text":"\ud83d\udcd6 Definition : Concurrency is the management of multiple tasks at the same time. In fastcubo , it is used to handle multiple image downloads efficiently. \ud83d\udcbb Example in fastcubo : ThreadPoolExecutor is used to manage concurrent downloads in getPixels and computePixels , allowing multiple images to be downloaded simultaneously, maximizing CPU and network usage.","title":"Concurrency \u2699\ufe0f"},{"location":"docs/Process/difference.html","text":"Difference between concurrency and parallelism \u2696\ufe0f \u2699\ufe0f Concurrency : Multiple tasks progress at the same time, but they do not necessarily run simultaneously. \ud83d\udda5\ufe0f Parallelism : Multiple tasks run at the same time on different CPU cores.","title":"**Difference between concurrency and parallelism** \u2696\ufe0f"},{"location":"docs/Process/difference.html#difference-between-concurrency-and-parallelism","text":"\u2699\ufe0f Concurrency : Multiple tasks progress at the same time, but they do not necessarily run simultaneously. \ud83d\udda5\ufe0f Parallelism : Multiple tasks run at the same time on different CPU cores.","title":"Difference between concurrency and parallelism \u2696\ufe0f"},{"location":"docs/Process/parallelism.html","text":"Parallelism \ud83d\udda5\ufe0f \ud83d\udcd6 Definition : Parallelism is the execution of multiple tasks simultaneously, each on its own CPU core. \ud83d\udcbb Example in fastcubo : If the system has multiple CPU cores, ThreadPoolExecutor allows several image downloads to run in parallel, reducing total download time.","title":"**Parallelism** \ud83d\udda5\ufe0f"},{"location":"docs/Process/parallelism.html#parallelism","text":"\ud83d\udcd6 Definition : Parallelism is the execution of multiple tasks simultaneously, each on its own CPU core. \ud83d\udcbb Example in fastcubo : If the system has multiple CPU cores, ThreadPoolExecutor allows several image downloads to run in parallel, reducing total download time.","title":"Parallelism \ud83d\udda5\ufe0f"},{"location":"docs/Process/recursion.html","text":"Recursion \ud83d\udd04 \ud83d\udcd6 Definition : Recursion is a technique where a function calls itself to solve smaller subproblems of the original problem. \ud83d\udcbb Example in fastcubo : Functions like computePixels_np and getPixels_np use recursion to split large images into manageable parts, downloading them in batches.","title":"**Recursion** \ud83d\udd04"},{"location":"docs/Process/recursion.html#recursion","text":"\ud83d\udcd6 Definition : Recursion is a technique where a function calls itself to solve smaller subproblems of the original problem. \ud83d\udcbb Example in fastcubo : Functions like computePixels_np and getPixels_np use recursion to split large images into manageable parts, downloading them in batches.","title":"Recursion \ud83d\udd04"}]}